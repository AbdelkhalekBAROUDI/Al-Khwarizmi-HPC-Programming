{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f03849c1",
   "metadata": {},
   "source": [
    "*Note: all those exercises should be done using python with pyccel openmp*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2db4b9c",
   "metadata": {},
   "source": [
    "## Exercise 1 Hello World\n",
    "\n",
    "1. Write an OpenMP program displaying the number of threads used for the execution and the rank of each of the threads.\n",
    "3. Test the programs obtained with different numbers of threads for the parallel program.\n",
    "\n",
    "**Output Example**\n",
    "```shell\n",
    ">> Python hello.py\n",
    "Hello from the rank 2 thread\n",
    "Hello from the rank 0 thread\n",
    "Hello from the rank 3 thread\n",
    "Hello from the rank 1 thread\n",
    "Hello from the rank 4 thread\n",
    "Parallel execution of hello_world with 4 threads\n",
    "```\n",
    "*Note that the output order maybe different*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8892a9e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing hello.py\n"
     ]
    }
   ],
   "source": [
    "%%file hello.py\n",
    "if __name__ == \"__main__\":\n",
    "    from pyccel.stdlib.internal.openmp import omp_get_num_threads, omp_get_thread_num\n",
    "    #$ omp parallel\n",
    "    print(\"Hello from the rank\", omp_get_thread_num(), \"thread\")\n",
    "    num_threads = omp_get_num_threads()\n",
    "\n",
    "    #$ omp end parallel\n",
    "    print(\"Parallel execution of hello_world with, \", num_threads, \"threads\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0005a1b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rakati/Teaching_dev/MSD/hpc-env/lib/python3.8/site-packages/pyccel/codegen/compiling/compilers.py:401: UserWarning: /home/rakati/Teaching_dev/MSD/hpc/day02/assignments/__pyccel__/prog_hello.c: In function ‘main’:\n",
      "/home/rakati/Teaching_dev/MSD/hpc/day02/assignments/__pyccel__/prog_hello.c:11:22: warning: format ‘%ld’ expects argument of type ‘long int’, but argument 3 has type ‘int’ [-Wformat=]\n",
      "   11 |         printf(\"%s %ld %s\\n\", \"Hello from the rank\", omp_get_thread_num(), \"thread\");\n",
      "      |                    ~~^                               ~~~~~~~~~~~~~~~~~~~~\n",
      "      |                      |                               |\n",
      "      |                      long int                        int\n",
      "      |                    %d\n",
      "\n",
      "  warnings.warn(UserWarning(err))\n",
      "Hello from the rank 1 thread\n",
      "Hello from the rank 7 thread\n",
      "Hello from the rank 2 thread\n",
      "Hello from the rank 4 thread\n",
      "Hello from the rank 6 thread\n",
      "Hello from the rank 0 thread\n",
      "Hello from the rank 5 thread\n",
      "Hello from the rank 3 thread\n",
      "Parallel execution of hello_world with,  8 threads\n"
     ]
    }
   ],
   "source": [
    "!pyccel hello.py --language=c --openmp; ./hello"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee0d684",
   "metadata": {},
   "source": [
    "## Exercise 2 Matrix product\n",
    "\n",
    "Considering the following code for matrix product:\n",
    "\n",
    "1. Using pyccel, epyccelize the `matrix_prod` function, and time the execution of epyccelized function.\n",
    "2. Insert the appropriate OpenMP directives and analyse the code performance.\n",
    "3. Test the loop iteration repartition modes (`STATIC`, `DYNAMIC`, `GUIDED`) and vary the chunk sizes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e94defd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Value of M and N           : 20 20 \n",
      "    Temps elapsed              : 0.0009012222290039062 \n",
      "    Temps CPU                  : 0.0 \n",
      "    Partiel results            : 2790.0 2690.0 ... -3610.0 -4350.0 \n",
      "\n",
      " Execution of Matrix production in parallele with                     8 threads\n"
     ]
    }
   ],
   "source": [
    "def matrix_prod(A:'real[:,:]', B:'real[:,:]', C:'real[:,:]', N:int, M:int):\n",
    "    from pyccel.stdlib.internal.openmp import omp_get_num_threads\n",
    "    #$ omp parallel\n",
    "    nb_threads = omp_get_num_threads()\n",
    "    #$ omp for schedule(runtime) nowait\n",
    "    for i in range( M ):\n",
    "        for j in range( N ):\n",
    "            A[ i, j ] = (i + 1) + (j + 1)\n",
    "\n",
    "    #$ omp for schedule(runtime) nowait\n",
    "    for i in range( N ):\n",
    "        for j in range( M ):\n",
    "            B[ i, j ] = (i + 1) - (j + 1)\n",
    "\n",
    "    #$ omp for schedule(runtime)\n",
    "    for i in range( M ):\n",
    "        for j in range( M ):\n",
    "            C[ i, j ] = 0\n",
    "    # ...\n",
    "\n",
    "    # Matrix Production\n",
    "    #$ omp for schedule(runtime) nowait\n",
    "    for i in range( M ):\n",
    "        for j in range( M ):\n",
    "            for k in range( N ):\n",
    "                C[ i, j ] += A[ i, k ] * B[ k, j ]\n",
    "\n",
    "    #$ omp end parallel\n",
    "    print(\"Execution of Matrix production in parallele with\",nb_threads, \"threads\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import numpy as np\n",
    "    import time\n",
    "    import math\n",
    "\n",
    "    M = 20\n",
    "    N = 20\n",
    "\n",
    "    # Declare Matrices\n",
    "    A = np.empty((M, N), dtype=np.double)\n",
    "    B = np.empty((N, M), dtype=np.double)\n",
    "    C = np.empty((M, M), dtype=np.double)\n",
    "\n",
    "    from pyccel.epyccel import epyccel\n",
    "    c_matrix_prod = epyccel(matrix_prod, language='fortran', accelerators=['openmp'])\n",
    "\n",
    "    # Start timing\n",
    "    tcpu_0 = time.process_time() # CPU time\n",
    "    t_elapsed_0 = time.time()    # Wall time\n",
    "\n",
    "    # -------------------- computing ------------------\n",
    "    c_matrix_prod(A, B, C, M, N)\n",
    "    # ------------------ End computing ----------------\n",
    "\n",
    "    # CPU time spent\n",
    "    tcpu_1 = time.process_time()\n",
    "    tcpu = tcpu_1 - tcpu_0\n",
    "\n",
    "    # Wall time spent\n",
    "    t_elapsed_1 = time.time()\n",
    "    t_elapsed = t_elapsed_1 - t_elapsed_0\n",
    "\n",
    "    # Print result\n",
    "    print(\n",
    "      \"   Value of M and N           :\", M, N,      \"\\n\",\n",
    "\t  \"   Temps elapsed              :\", t_elapsed, \"\\n\",\n",
    "\t  \"   Temps CPU                  :\", tcpu,      \"\\n\",\n",
    "      \"   Partiel results            :\", C[ 1, 1 ], C[ 2, 2 ], \"...\", C[ M-3, M-3 ], C[ M-2, M-2 ], \"\\n\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db729e9b",
   "metadata": {},
   "source": [
    "## Exercise 3 Jacobi method\n",
    "\n",
    "Considering the following code for a general linear system solver:\n",
    "\n",
    "$$ A \\times x = b $$\n",
    "\n",
    "using the Jacobi iterative method.\n",
    "In this exercice, you must solve the system in parallel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8af86ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System size    : 20 \n",
      " Iterations     : 21 \n",
      " Stand          : 2.9100049961372497e-08 \n",
      " Elapsed Time   : 2.1162259578704834 \n",
      " CPU time       : 0.125 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def jacobi(a:'double[:,:]', b:'double[:]', x:'double[:]', n:int, eps:'double'):\n",
    "    from pyccel.stdlib.internal.openmp import omp_get_num_threads\n",
    "    #$ omp parallel\n",
    "    nb_tasks = omp_get_num_threads()\n",
    "    print(\"Execution of Jacobi in parallel with\", nb_tasks, \"threads\")\n",
    "    #$ omp end parallel\n",
    "    # Jacobi method resolution\n",
    "    import numpy as np\n",
    "    import math\n",
    "    x_courant = np.empty(n, dtype=np.double)\n",
    "    iteration = 0\n",
    "    while(1):\n",
    "        iteration += 1\n",
    "\n",
    "        #$ omp parallel for schedule(runtime)\n",
    "        for i in range(n):\n",
    "            x_courant[i] = 0\n",
    "            for j in range(i):\n",
    "                x_courant[i] += a[ j, i ] * x[j]\n",
    "\n",
    "            for j in range(i + 1, n):\n",
    "                x_courant[i] += a[ j, i ] * x[j]\n",
    "\n",
    "            x_courant[i] = (b[i] - x_courant[i]) / a[ i, i ]\n",
    "\n",
    "        # Convergence test\n",
    "        absmax = 0.\n",
    "        for i in range(n):\n",
    "            curr = math.fabs(x[i] - x_courant[i])\n",
    "            if curr > absmax:\n",
    "                absmax = curr\n",
    "        stand = absmax / n\n",
    "        if stand <= eps or iteration > n:\n",
    "            break\n",
    "\n",
    "        # copy x_courant into x\n",
    "        for i in range(n):\n",
    "            x [i] = x_courant[i]\n",
    "    return stand, iteration\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    import numpy as np\n",
    "    import time, random, math, sys\n",
    "\n",
    "    # Matrix default dimension\n",
    "    n = 20\n",
    "    diag = 20\n",
    "\n",
    "    # Initialize of the matrices\n",
    "    np.random.seed(421)  # for reproductibles results\n",
    "    a = np.random.uniform(0, 1, size=(n, n))\n",
    "    b = np.random.uniform(0, 1, size=n)\n",
    "\n",
    "    # We strengthen the main diagonal of the matrix\n",
    "    for i in range(n):\n",
    "        a[ i, i ] += diag\n",
    "\n",
    "    # Initial solution\n",
    "    x = np.ones(n, dtype=np.double)\n",
    "\n",
    "    # start CPU timing\n",
    "    cpu_0 = time.process_time()\n",
    "\n",
    "    # start Wall timing\n",
    "    elapsed_0 = time.time()\n",
    "\n",
    "    # Pyccelize jacobi function\n",
    "    from pyccel.epyccel import epyccel\n",
    "    p_jacobi = epyccel(jacobi, language='c', accelerators=['openmp'])\n",
    "\n",
    "    eps = sys.float_info.epsilon\n",
    "\n",
    "    # -------------------- computing ------------------\n",
    "    stand, iteration = p_jacobi(a, b, x, n, eps)\n",
    "    # ------------------ End computing ----------------\n",
    "\n",
    "    # CPU time\n",
    "    cpu_1 = time.process_time()\n",
    "    cpu = cpu_1 - cpu_0\n",
    "\n",
    "    # Wall timing\n",
    "    elapsed_1 = time.time()\n",
    "    elapsed = elapsed_1 - elapsed_0\n",
    "\n",
    "    print(\n",
    "        \"System size    :\", n,          '\\n',\n",
    "        \"Iterations     :\", iteration,  '\\n',\n",
    "        \"Stand          :\", stand,      '\\n',\n",
    "        \"Elapsed Time   :\", elapsed,    '\\n',\n",
    "        \"CPU time       :\", cpu,        '\\n'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd992ae",
   "metadata": {},
   "source": [
    "## Exercise 4 Calculation of π\n",
    "\n",
    "The aim of this exercise is to calculate π by numerical integration knowing that Considering the following code for matrix product: $\\int_{0}^{1} \\frac{4}{1 + x^2} = {\\pi} $\n",
    "\n",
    "The following program is for calculating the value of\n",
    "π by the rectangle method (mid-point). Let $f(x) = \\frac{4}{1 + x^2} $\n",
    "2 be the function to integrate, N and $ h = \\frac{1}{N} $ (respectively) the number of points, and the discretization width on the integration\n",
    "interval $[0, 1]$.\n",
    "\n",
    "This exercice can be parallelized in three different ways (i.e. using different **OpenMP** directives for each version). Analyse the performance of the three codes, then optimise the least efficient versions (without changing the type of OpenMP directives used), in order to obtain the same performance for the three parallelized versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c2773e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing pi.py\n"
     ]
    }
   ],
   "source": [
    "%%file pi.py\n",
    "def f(a:'double'):\n",
    "    return 4.0 / (1. + a * a)\n",
    "\n",
    "def pi(n:int, h:'double'):\n",
    "\n",
    "\n",
    "    from pyccel.stdlib.internal.openmp import omp_get_num_threads\n",
    "    #$ omp parallel\n",
    "    nb_tasks = omp_get_num_threads()\n",
    "    #$ omp end parallel\n",
    "    print(\"Execution of PI in parallel with\", nb_tasks,\"threads\")\n",
    "\n",
    "    for k in range(100):\n",
    "        Pi_calc = 0.\n",
    "        #$ omp parallel for reduction(+: Pi_calc) schedule(runtime)\n",
    "        for i in range(n):\n",
    "            x = h * (i + 0.5)\n",
    "            Pi_calc += f(x)\n",
    "\n",
    "        Pi_calc = h * Pi_calc\n",
    "    return Pi_calc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "38eb3ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Number of intervals        : 300 \n",
      "    | Pi_estime - Pi_calcule | : 9.259259270422149e-07 \n",
      "    Temps elapsed              : 0.0015625953674316406 \n",
      "    Temps CPU                  : 0.0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time, math\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #                  __\n",
    "    #  But : calcul de || par la methode des rectangles (point milieu).\n",
    "    #\n",
    "    #                   / 1\n",
    "    #                  |       4            __\n",
    "    #                  |   ----------  dx = ||\n",
    "    #                  |    1 + x**2\n",
    "    #                 / 0\n",
    "\n",
    "\n",
    "    n = 300\n",
    "\n",
    "    # Length of the integration interval\n",
    "    h = 1.0 / n\n",
    "\n",
    "    from pyccel.epyccel import epyccel\n",
    "    import pi\n",
    "    pyc_pi = epyccel(pi, language='c', accelerators=['openmp'])\n",
    "\n",
    "    # start timing\n",
    "    tcpu_0 = time.process_time() # CPU time\n",
    "    t_elapsed_0 = time.time()    # Wall time\n",
    "\n",
    "    # -------------------- computing ------------------\n",
    "\n",
    "    Pi_calc = pyc_pi.pi(n, h)\n",
    "\n",
    "    # ------------------ End computing ----------------\n",
    "\n",
    "    # CPU time spent\n",
    "    tcpu_1 = time.process_time()\n",
    "    tcpu = tcpu_1 - tcpu_0\n",
    "\n",
    "    # Wall time spent\n",
    "    t_elapsed_1 = time.time()\n",
    "    t_elapsed = t_elapsed_1 - t_elapsed_0\n",
    "\n",
    "    # deviation between the estimated value and the calculated value of Pi\n",
    "    Pi_estime = math.acos(-1)\n",
    "    deviat = math.fabs(Pi_estime - Pi_calc)\n",
    "\n",
    "    # Print result\n",
    "    print(\n",
    "      \"   Number of intervals        :\", n,         \"\\n\",\n",
    "\t  \"   | Pi_estime - Pi_calcule | :\", deviat,    \"\\n\",\n",
    "\t  \"   Temps elapsed              :\", t_elapsed, \"\\n\",\n",
    "\t  \"   Temps CPU                  :\", tcpu,      \"\\n\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5f639d",
   "metadata": {},
   "source": [
    "## Exercise 5 The conjugate gradient method\n",
    "\n",
    "The program contained in the gradient conjugue.f90 file solves a symmetric linear system\n",
    "\n",
    "$$ A \\times x = b $$\n",
    "\n",
    "using the preconditioned conjugate gradient method. In Python, this program can be parallelized primarily by using the WORKSHARE constructions.\n",
    "1. After introducing the appropriate OpenMP directives, analyse the code performance.\n",
    "2. What are your conclusions about the effectiveness of the WORKSHARE directive ?\n",
    "3. Optimise the parallel version of the code by slightly modifying the source code in order to avoid using the WORKSHARE directive in places where it is problematic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5591c8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.80595766 0.98699575 0.98699469 ... 0.48088396 0.70539741 0.35324892]\n",
      " [0.98699575 0.64720747 0.31109875 ... 0.43588759 0.54116159 0.18821353]\n",
      " [0.98699469 0.31109875 0.13036537 ... 0.49731452 0.60446757 0.30887318]\n",
      " ...\n",
      " [0.48088396 0.43588759 0.49731452 ... 0.97571607 0.42813009 0.35654696]\n",
      " [0.70539741 0.54116159 0.60446757 ... 0.42813009 0.65288483 0.44419889]\n",
      " [0.35324892 0.18821353 0.30887318 ... 0.35654696 0.44419889 0.0127382 ]]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'z' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_193/3673780248.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mcg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_193/3673780248.py\u001b[0m in \u001b[0;36mcg\u001b[0;34m(A, B, n)\u001b[0m\n\u001b[1;32m     34\u001b[0m       \u001b[0;31m#$ omp end single\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m       \u001b[0;31m#$ omp workshare\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m       \u001b[0;31m#$ omp end workshare\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'z' is not defined"
     ]
    }
   ],
   "source": [
    "def cg(A:'double[:,:]', B:'double[:]', n:int):\n",
    "#$ omp parallel\n",
    "\n",
    "    # On symmetrise la matrice\n",
    "  #$ omp workshare\n",
    "    for i in range(1, n):\n",
    "        for j in range(n - 1):\n",
    "            A[ i, j ] = A[ j, i ]\n",
    "\n",
    "    print(A)\n",
    "    # We strengthen the main diagonal\n",
    "    for i in range(n):\n",
    "        A[ i, i ] += 800.0\n",
    "\n",
    "    pre = np.empty((n), dtype=np.double)\n",
    "    # We choose the Jacobi preconditioning = Diagonal (a)\n",
    "    for i in range(n):\n",
    "        pre[i] = A[ i, i ]\n",
    "\n",
    "    # Initial solution\n",
    "    x = np.ones((n), dtype=np.double)\n",
    "\n",
    "    # Initial ecart\n",
    "    r = B[:] - np.matmul(A, x)\n",
    "  #$ omp end workshare\n",
    "\n",
    "    # Resolution using gradient conjugue methode\n",
    "    iteration = 0\n",
    "    while True:\n",
    "      #$ omp single\n",
    "        rho = 0.0\n",
    "        gamma = 0.0\n",
    "        iteration += 1\n",
    "      #$ omp end single\n",
    "      #$ omp workshare\n",
    "#         not completed !!!!!!!!!\n",
    "\n",
    "      #$ omp end workshare\n",
    "\n",
    "#$ omp end parallel\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import numpy as np\n",
    "    n = 4201\n",
    "\n",
    "    # Initialize Matrices\n",
    "    A = np.random.uniform(0, 1, size=(n, n))\n",
    "    B = np.random.uniform(0, 1, size=(n, n))\n",
    "    cg(A, B, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e9a63a",
   "metadata": {},
   "source": [
    "## Exercise 6 Reduction of an array\n",
    "\n",
    "The given program contained in the `reduction_tab.py` file is extracted from a chemistry code. It reduces a three-dimensional array into a vector. The aim of this exercise is to parallelize this calculation kernel without changing the loop order in the provided code (i.e. k,j,i)\n",
    "\n",
    "1. Analyse the data-sharing attributes of the variables and adapt the source code so that the K outermost loop is parallelized.\n",
    "2. Compare the performance obtained by using the thread/core binding default execution on Ada and by using scatter binding. Suggest an explanation for the poor performance of the latter.\n",
    "3. Optimise the source code for the scatter mode with taking into account the memory affinity. Why does this third series of executions give the best performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b9fdd28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing reduction_tab.py\n"
     ]
    }
   ],
   "source": [
    "%%file reduction_tab.py\n",
    "\n",
    "def threads_num():\n",
    "    from pyccel.stdlib.internal.openmp import omp_get_num_threads\n",
    "    #$ omp parallel\n",
    "    nb_taches = omp_get_num_threads()\n",
    "    #$ omp end parallel\n",
    "    return nb_taches\n",
    "\n",
    "def initialize_table(tab:'double[:,:,:]', nmolec:int, n:int, nmol:int):\n",
    "    # Initialisation du tableau\n",
    "    # First-touch pour garantir un fonctionnement optimal sur les syst�mes NUMA\n",
    "    #$ omp for schedule(static)\n",
    "    for k in range(nmolec):\n",
    "        for j in range(n):\n",
    "            for i in range(nmol):\n",
    "                tab[ i,j,k ] = i + j + k\n",
    "\n",
    "def reduction(tab:'double[:,:,:]', tab1:'double[:]', tab2:'double[:]', nmolec:int, nmol:int, n:int):\n",
    "    #$ omp parallel private(tab1)\n",
    "    #$ omp for schedule(static) reduction(+:tab2)\n",
    "    for k in range(nmolec):\n",
    "        tab1[:nmol] = 0\n",
    "        for j in range(n):\n",
    "            for i in range(nmol):\n",
    "                tab1[i] = tab1[i] + tab[i,j,k]\n",
    "        tab2[:nmol] = tab2[:nmol] + 2 * tab1[:nmol]\n",
    "    #$ omp end parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "659bd746",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution of reduction table in parallele with <built-in function threads_num> threads\n",
      "Temps elapsed  : 0.0009007453918457031 \n",
      " Temps CPU      : 0.0 \n",
      " Erreur relative: 0.0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from os import error\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import numpy as np\n",
    "    import time\n",
    "\n",
    "    # default dimension\n",
    "    nmolec = 100\n",
    "    nmol   = 100\n",
    "    n      = 10\n",
    "\n",
    "    tab     = np.empty((nmol, n, nmolec), dtype=np.double)\n",
    "    tab1    = np.empty((nmol), dtype=np.double)\n",
    "    tab2    = np.zeros((nmol), dtype=np.double)\n",
    "\n",
    "    # pyccelize reduction tab module\n",
    "    from pyccel.epyccel import epyccel\n",
    "    import reduction_tab\n",
    "    \n",
    "    reduction = epyccel(reduction_tab, language='fortran', accelerators=['openmp'])\n",
    "    \n",
    "    print(\"Execution of reduction table in parallele with\", reduction.threads_num, \"threads\")\n",
    "\n",
    "    # initialize tab \n",
    "    reduction.initialize_table(tab, nmol, n, nmolec)\n",
    "\n",
    "    # start timing\n",
    "    tcpu_0 = time.process_time() # CPU time\n",
    "    t_elapsed_0 = time.time()    # Wall time\n",
    "\n",
    "    # -------------------- computing ------------------\n",
    "    reduction.reduction(tab, tab1, tab2, nmolec, nmol, n)\n",
    "    # ------------------ End computing ----------------\n",
    "\n",
    "    # CPU time spent\n",
    "    tcpu_1 = time.process_time()\n",
    "    tcpu = tcpu_1 - tcpu_0\n",
    "\n",
    "    # Wall time spent\n",
    "    t_elapsed_1 = time.time()\n",
    "    t_elapsed = t_elapsed_1 - t_elapsed_0\n",
    "\n",
    "    # verification of results\n",
    "    tab2c = np.zeros((nmol), dtype=np.double)\n",
    "    for k in range(nmolec):\n",
    "        tab1c = np.zeros((nmol), dtype=np.double)\n",
    "        for j in range(n):\n",
    "            for i in range(nmol):\n",
    "                tab1c[i] = tab1c[i] + tab[i, j, k]\n",
    "        tab2c [:] = tab2c[:nmol] + 2 * tab1c[:]\n",
    "    err = np.max(np.abs(tab2c - tab2) / np.abs(tab2c))\n",
    "    print(\n",
    "        \"Temps elapsed  :\", t_elapsed   , \"\\n\",\n",
    "        \"Temps CPU      :\", tcpu        , \"\\n\",\n",
    "        \"Erreur relative:\", err         , \"\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4045d2e1",
   "metadata": {},
   "source": [
    "# Exercise 7 Matrix product by the Strassen algorithm\n",
    "\n",
    "Considering the following code that calculates the matrix product :\n",
    "\n",
    "$$C = A \\times B$$\n",
    "\n",
    "by using Strassen’s recursive algorithm.\n",
    "\n",
    "In this exercise, you must :\n",
    "1. Analyse and parallelize the code by using OpenMP tasks.\n",
    "2. Measure the code performance and plot the speedup curves obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bd9c8a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Erreur        : 4.962553327910702e-14 \n",
      "    Temps elapsed : 0.0027666091918945312 \n",
      "    Temps CPU     : 0.0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time, math\n",
    "import numpy as np\n",
    "\n",
    "def threads_num():\n",
    "    from pyccel.stdlib.internal.openmp import omp_get_num_threads\n",
    "    #$ omp parallel\n",
    "    nb_taches = omp_get_num_threads()\n",
    "    #$ omp end parallel\n",
    "    return nb_taches\n",
    "\n",
    "\n",
    "def strassen_multiply(A:'double[:, :]', B:'double[:, :]', C:'double[:, :]', n:int):\n",
    "    import numpy as np\n",
    "    if n & 1 != 0  or n < 128:\n",
    "        C[:] = np.matmul(A, B)\n",
    "    else:\n",
    "        n2 = n // 2\n",
    "        A11 = A[ 0:n2, 0:n2 ]\n",
    "        A21 = A[ n2:n, 0:n2 ]\n",
    "        A12 = A[ 0:n2, n2:n ]\n",
    "        A22 = A[ n2:n, n2:n ]\n",
    "        B11 = B[ 0:n2, 0:n2 ]\n",
    "        B21 = B[ n2:n, 0:n2 ]\n",
    "        B12 = B[ 0:n2, n2:n ]\n",
    "        B22 = B[ n2:n, n2:n ]\n",
    "\n",
    "        Q1 = np.empty((n2, n2), dtype=np.double)\n",
    "        Q2 = np.empty((n2, n2), dtype=np.double)\n",
    "        Q3 = np.empty((n2, n2), dtype=np.double)\n",
    "        Q4 = np.empty((n2, n2), dtype=np.double)\n",
    "        Q5 = np.empty((n2, n2), dtype=np.double)\n",
    "        Q6 = np.empty((n2, n2), dtype=np.double)\n",
    "        Q7 = np.empty((n2, n2), dtype=np.double)\n",
    "        \n",
    "        #$ omp task shared(A11,A22,B11,B22,Q1,n) \n",
    "        strassen_multiply(A11+A22, B11+B22, Q1, n2)\n",
    "        #$ omp end task\n",
    "        #$ omp task shared(A21,A22,B11,Q2,n)\n",
    "        strassen_multiply(A21+A22, B11, Q2, n2)\n",
    "        #$ omp end task\n",
    "        #$ omp task shared(A11,B12,B22,Q3,n) \n",
    "        strassen_multiply(A11, B12-B22, Q3, n2)\n",
    "        #$ omp end task\n",
    "        #$ omp task shared(A22,B11,B21,Q4,n) \n",
    "        strassen_multiply(A22, -B11+B21, Q4, n2)\n",
    "        #$ omp end task\n",
    "        #$ omp task shared(A11,A12,B22,Q5,n) \n",
    "        strassen_multiply(A11+A12, B22, Q5, n2)\n",
    "        #$ omp end task\n",
    "        #$ omp task shared(A11,A21,B11,B12,Q6,n) \n",
    "        strassen_multiply(-A11+A21, B11+B12, Q6, n2)\n",
    "        #$ omp end task\n",
    "        #$ omp task shared(A12,A22,B21,B22,Q7,n)\n",
    "        strassen_multiply(A12-A22, B21+B22, Q7, n2)\n",
    "        #$ omp end task\n",
    "        #$ omp taskwait\n",
    "        C[   :n2,   :n2 ] = Q1+Q4-Q5+Q7\n",
    "        C[ n2: n,   :n2 ] = Q2+Q4\n",
    "        C[   :n2, n2:n  ] = Q3+Q5\n",
    "        C[ n2:n , n2:n  ] = Q1+Q3-Q2+Q6\n",
    "\n",
    "\n",
    "def calcul_erreur(n:int, A:'double[:, :]', B:'double[:, :]', C:'double[:, :]'):\n",
    "    D = np.matmul(A, B) \n",
    "    error = 0.0\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            e = C[ i, j ] - D[ i, j ]\n",
    "            error += e * e\n",
    "\n",
    "    error = math.sqrt(error) / n\n",
    "    return error\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    n = 256\n",
    "\n",
    "    # Initialize Matrices\n",
    "    np.random.seed(421)\n",
    "    A = np.random.uniform(0, 1, (n, n))\n",
    "    B = np.random.uniform(0, 1, (n, n))\n",
    "    C = np.empty((n,n), dtype=np.double)\n",
    "\n",
    "    from pyccel.epyccel import epyccel\n",
    "    strassen = epyccel(strassen_multiply, accelerators=['openmp'])\n",
    "    num_threads = epyccel(threads_num, accelerators=['openmp'])\n",
    "    print(\"Execution of Strassen in parallele with\", num_threads(), \"threads\")\n",
    "    \n",
    "    \n",
    "    # Start timing\n",
    "    tcpu_0 = time.process_time() # CPU time\n",
    "    t_elapsed_0 = time.time()    # Wall time\n",
    "    # -------------------- computing ------------------\n",
    "    # Calcul C=A*B par la methode recursive de Strassen\n",
    "    strassen(A, B, C, n)\n",
    "    # ------------------ end computing ----------------\n",
    "    # CPU time spent\n",
    "    tcpu_1 = time.process_time()\n",
    "    tcpu = tcpu_1 - tcpu_0\n",
    "\n",
    "    # Wall time spent\n",
    "    t_elapsed_1 = time.time()\n",
    "    t_elapsed = t_elapsed_1 - t_elapsed_0\n",
    "    # Print result\n",
    "    print(\n",
    "      \"   Erreur        :\", calcul_erreur(n, A, B, C),  \"\\n\",\n",
    "\t  \"   Temps elapsed :\", t_elapsed,                  \"\\n\",\n",
    "\t  \"   Temps CPU     :\", tcpu,                       \"\\n\",\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
